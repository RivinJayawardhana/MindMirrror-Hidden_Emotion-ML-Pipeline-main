data_paths:
  # Default dataset (can be overridden via command-line)
  raw_data: "merged_full_dataset.csv"
  processed_data: "data/processed/preprocessed_emotions.csv"
  artifacts_dir: "artifacts"
  data_artifacts_dir: "artifacts/data"
  model_artifacts_dir: "artifacts/models"
  mlflow_tracking_uri: "file:./mlruns"
  pretrained_models_dir: "artifacts/pretrained"  # HuggingFace models saved here for reuse
  
  # Multiple dataset support (optional - use dataset_name to select)
  datasets:
    default: "merged_full_dataset.csv"
    merged: "merged_full_dataset.csv"
    balanced: "balanced_emotion_dataset_smart.csv"
    clean: "balanced_emotion_dataset_smart_clean_text.csv"
    # Add your custom datasets here:
    # custom1: "path/to/your/dataset1.csv"
    # custom2: "path/to/your/dataset2.csv"

attributes:
  emotion_categories:
    - 'joy'
    - 'anger'
    - 'sadness'
    - 'fear'
    - 'love'
    - 'surprise'
  required_attributes:
    - 'text'
    - 'hidden_emotion_label'
    - 'primary_emoji'
    - 'emoji_emotion'
  text_column: 'text'
  label_column: 'hidden_emotion_label'

model:
  base_model_name: "microsoft/deberta-v3-base"  # Can be changed to roberta-base, bert-base-uncased, etc.
  num_emotions: 6
  dropout: 0.3
  freeze_layers: 1  # Reduced from 2 - allow more layers to train

training:
  num_epochs: 12                    # +2 epochs, let early stopping decide
  batch_size: 24                    # ↓16, more stable gradients
  val_batch_size: 64                # keep
  
  # ↑ Learning rates (your 5e-6 too low for DeBERTa)
  learning_rate_encoder: 2e-5       # standard DeBERTa fine-tune LR
  learning_rate_head: 5e-5          # 2.5x encoder (heads adapt faster)
  
  weight_decay: 0.01                # keep
  max_grad_norm: 1.0                # keep
  
  warmup_steps: 0.08                # ↓2%, faster ramp-up
  scheduler_type: "cosine"          # keep
  
  # Early stopping - better metric
  early_stopping:
    enabled: true
    patience: 4                     # ↓1, more responsive
    monitor: "val_emo_macro_f1"     # ↑ F1 > accuracy for imbalance
    mode: "max"
  
  # Class weights - more aggressive
  class_weights:
    method: "effective_num"
    beta: 0.99                      # ↓0.009, stronger reweighting
  
  # Loss - more focused
  loss:
    focal_gamma: 2.0                # ↑0.5, harder on easy examples
    hidden_weight: 0.8              # ↑0.3, hidden task more important
    label_smoothing: 0.02           # ↓0.03, less aggressive
    pos_weight: 2.2                 # ↑0.7, hidden minority class
  
  # Augmentation - more aggressive on minorities
  augmentation:
    enabled: true
    probability: 0.4                # ↑0.1
    minority_classes: [2, 3, 4, 5]  # add anger (usually imbalanced)
  
  # Data split - keep stable
  train_val_split:
    test_size: 0.2
    random_state: 42
    stratify: true

tokenizer:
  max_length: 128
  padding: "max_length"
  truncation: true

mlflow:
  experiment_name: "hidden_emotion_detection"
  tracking_enabled: true
  log_artifacts: true