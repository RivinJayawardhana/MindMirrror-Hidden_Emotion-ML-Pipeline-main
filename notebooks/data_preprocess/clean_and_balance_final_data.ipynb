{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7a971b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                          6755\n",
       "hidden_emotion_label          6755\n",
       "hidden_emotion_flag           6755\n",
       "emoji_text_sentiment_match    6755\n",
       "emotion_label                 6755\n",
       "primary_emoji                 6755\n",
       "emoji_emotion                 4755\n",
       "hidden_emotion_id             6755\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('merged_training_dataset.csv')\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1066a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EMOTION DATASET CLEANING PIPELINE\n",
      "======================================================================\n",
      "Preserving all original columns with cleaned data\n",
      "\n",
      "ðŸ“ STEP 1: LOADING DATASET\n",
      "--------------------------------------------------\n",
      "âœ“ Dataset loaded successfully!\n",
      "  â€¢ Samples: 6,755\n",
      "  â€¢ Features: 8\n",
      "\n",
      "ðŸ“‹ ORIGINAL COLUMNS:\n",
      "  â€¢ text                          : 6755/6755 (100.0% non-null)\n",
      "  â€¢ hidden_emotion_label          : 6755/6755 (100.0% non-null)\n",
      "  â€¢ hidden_emotion_flag           : 6755/6755 (100.0% non-null)\n",
      "  â€¢ emoji_text_sentiment_match    : 6755/6755 (100.0% non-null)\n",
      "  â€¢ emotion_label                 : 6755/6755 (100.0% non-null)\n",
      "  â€¢ primary_emoji                 : 6755/6755 (100.0% non-null)\n",
      "  â€¢ emoji_emotion                 : 4755/6755 (70.4% non-null)\n",
      "  â€¢ hidden_emotion_id             : 6755/6755 (100.0% non-null)\n",
      "\n",
      "Sample of original data:\n",
      "                                                text hidden_emotion_label  \\\n",
      "0  \"Woah did you draw that?\"\\nNo, I Printed it. O...                  joy   \n",
      "1                       I CANT WITH THESE COMMENTSðŸ˜­ðŸ™                anger   \n",
      "2  \" Oh sorry, were you sleeping? \" No, I'm summo...                  joy   \n",
      "\n",
      "   hidden_emotion_flag  emoji_text_sentiment_match emotion_label  \\\n",
      "0                    1                           0         anger   \n",
      "1                    1                           0       sadness   \n",
      "2                    0                           1         anger   \n",
      "\n",
      "  primary_emoji emoji_emotion  hidden_emotion_id  \n",
      "0             ðŸ‘¹         anger                  2  \n",
      "1             ðŸ˜­       sadness                  0  \n",
      "2             ðŸ’€         anger                  2  \n",
      "\n",
      "ðŸ§¹ STEP 2: CLEANING DATA\n",
      "--------------------------------------------------\n",
      "Original samples: 6,755\n",
      "Preserving all 8 columns\n",
      "\n",
      "1. Cleaning 'text' column...\n",
      "\n",
      "2. Cleaning emotion label columns...\n",
      "  â€¢ hidden_emotion_label     : 6755/6755 cleaned\n",
      "  â€¢ emotion_label            : 6755/6755 cleaned\n",
      "  â€¢ emoji_emotion            : 4755/4755 cleaned\n",
      "\n",
      "3. Resolving final emotion label...\n",
      "\n",
      "4. Creating analysis features...\n",
      "\n",
      "5. Processing hidden_emotion_id...\n",
      "  â€¢ Found 6 unique emotion IDs\n",
      "\n",
      "6. Removing duplicates...\n",
      "\n",
      "7. Filtering short texts...\n",
      "\n",
      "8. Encoding final emotion labels...\n",
      "\n",
      "ðŸ” STEP 3: QUALITY FILTERING\n",
      "--------------------------------------------------\n",
      "\n",
      "9. Calculating confidence scores...\n",
      "\n",
      "âš–ï¸  STEP 4: BALANCING CLASSES\n",
      "--------------------------------------------------\n",
      "Current class distribution:\n",
      "  â€¢ joy         : 1858 samples ( 31.4%)\n",
      "  â€¢ anger       : 1696 samples ( 28.7%)\n",
      "  â€¢ sadness     : 1197 samples ( 20.2%)\n",
      "  â€¢ fear        :  464 samples (  7.8%)\n",
      "  â€¢ love        :  426 samples (  7.2%)\n",
      "  â€¢ surprise    :  275 samples (  4.6%)\n",
      "\n",
      "Imbalance ratio: 6.8:1\n",
      "\n",
      "âš ï¸  Class imbalance detected! Applying balancing...\n",
      "  â†“ joy         : 1858 â†’  830 (undersampled)\n",
      "  â†“ anger       : 1696 â†’  830 (undersampled)\n",
      "  â†“ sadness     : 1197 â†’  830 (undersampled)\n",
      "  â†‘ fear        :  464 â†’  830 (oversampled)\n",
      "  â†‘ love        :  426 â†’  830 (oversampled)\n",
      "  â†‘ surprise    :  275 â†’  830 (oversampled)\n",
      "\n",
      "âœ“ Balancing complete!\n",
      "\n",
      "ðŸ“Š STEP 5: ORGANIZING FINAL DATASET\n",
      "--------------------------------------------------\n",
      "Final dataset has 4,980 samples and 25 columns\n",
      "\n",
      "Column groups in final dataset:\n",
      "  â€¢ IDENTIFIERS: 2 columns\n",
      "  â€¢ EMOTION_LABELS: 8 columns\n",
      "  â€¢ METADATA: 4 columns\n",
      "  â€¢ EMOJI_INFO: 4 columns\n",
      "  â€¢ TEXT_FEATURES: 7 columns\n",
      "\n",
      "âœ… STEP 6: QUALITY ASSURANCE\n",
      "--------------------------------------------------\n",
      "âœ“ No missing values in critical columns\n",
      "âœ“ No empty texts\n",
      "âœ— Duplicate texts: 1325\n",
      "âœ“ Minimum text length: 5 (OK)\n",
      "âœ“ Minimum class size: 830 (OK)\n",
      "âœ“ All emotion labels are valid\n",
      "\n",
      "Quality checks: 5/6 passed\n",
      "\n",
      "ðŸ’¾ STEP 7: SAVING DATASETS\n",
      "--------------------------------------------------\n",
      "âœ“ Perfect training dataset saved: perfect_training_dataset_full.csv\n",
      "  â€¢ Samples: 4,980\n",
      "  â€¢ Columns: 25\n",
      "âœ“ Label mapping saved: emotion_label_mapping.csv\n",
      "âœ“ Simplified dataset saved: perfect_training_dataset_simple.csv\n",
      "âœ“ Cleaning log saved: cleaning_log.csv\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š DATASET STATISTICS:\n",
      "  â€¢ Total samples: 4,980\n",
      "  â€¢ Unique emotions: 6\n",
      "  â€¢ Average text length: 75.0 chars\n",
      "  â€¢ Average word count: 13.2 words\n",
      "  â€¢ Samples with emojis: 4,980 (100.0%)\n",
      "  â€¢ Average confidence: 1.64\n",
      "\n",
      "ðŸŽ­ EMOTION DISTRIBUTION:\n",
      "  â€¢ anger       :  830 samples ( 16.7%)\n",
      "  â€¢ love        :  830 samples ( 16.7%)\n",
      "  â€¢ sadness     :  830 samples ( 16.7%)\n",
      "  â€¢ fear        :  830 samples ( 16.7%)\n",
      "  â€¢ joy         :  830 samples ( 16.7%)\n",
      "  â€¢ surprise    :  830 samples ( 16.7%)\n",
      "\n",
      "ðŸ“ SAVED FILES:\n",
      "  1. perfect_training_dataset_full.csv - Full dataset with all columns (4,980 samples)\n",
      "  2. perfect_training_dataset_simple.csv - Simplified version\n",
      "  3. emotion_label_mapping.csv - Emotion to ID mapping\n",
      "  4. cleaning_log.csv - Cleaning process log\n",
      "\n",
      "======================================================================\n",
      "âœ… PIPELINE COMPLETE! Perfect dataset is ready for training!\n",
      "======================================================================\n",
      "\n",
      "ðŸ‘€ SAMPLE OF FINAL DATASET:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ Sample 1:\n",
      "  Text: wait till she reads ig comments ðŸ’€\n",
      "  Final Emotion: anger (ID: 0)\n",
      "  Hidden Emotion: joy\n",
      "  Primary Emotion: anger\n",
      "  Emoji Emotion: anger\n",
      "  Confidence: 1.45\n",
      "  Length: 33 chars, 7 words\n",
      "  Emojis: ðŸ’€\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ Sample 2:\n",
      "  Text: Update: be right back pet care. appreciate you #shaking m...\n",
      "  Final Emotion: love (ID: 3)\n",
      "  Hidden Emotion: love\n",
      "  Primary Emotion: love\n",
      "  Confidence: 1.82\n",
      "  Length: 69 chars, 12 words\n",
      "  Emojis: â¤ï¸â¤ï¸\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ Sample 3:\n",
      "  Text: I love when people do their job half way &amp; I have to ...\n",
      "  Final Emotion: anger (ID: 0)\n",
      "  Hidden Emotion: anger\n",
      "  Primary Emotion: anger\n",
      "  Emoji Emotion: anger\n",
      "  Confidence: 1.82\n",
      "  Length: 148 chars, 29 words\n",
      "  Emojis: ðŸ˜¡\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š DATASET ANALYSIS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ“‹ COLUMN INFORMATION:\n",
      "Column                         Non-Null   Dtype      Description\n",
      "----------------------------------------------------------------------\n",
      "text                           4980       object     Original text\n",
      "text_clean                     4980       object     Cleaned text (main feature)\n",
      "hidden_emotion_label           4980       object     Original hidden emotion label\n",
      "hidden_emotion_label_clean     4980       object     Cleaned hidden emotion label\n",
      "emotion_label                  4980       object     Original primary emotion label\n",
      "emotion_label_clean            4980       object     Cleaned primary emotion label\n",
      "emoji_emotion                  2436       object     Original emoji emotion\n",
      "emoji_emotion_clean            2436       object     Cleaned emoji emotion\n",
      "final_emotion                  4980       object     Resolved final emotion label\n",
      "final_emotion_id               4980       int64      Encoded emotion ID\n",
      "hidden_emotion_flag            4980       int64      Flag for hidden emotion (0/1)\n",
      "emoji_text_sentiment_match     4980       int64      Emoji-text match flag (0/1)\n",
      "hidden_emotion_id              4980       int64      Hidden emotion ID\n",
      "confidence_score               4980       float64    Confidence score (0-2)\n",
      "primary_emoji                  4980       object     Primary emoji from text\n",
      "emojis_extracted               4980       object     All emojis extracted from text\n",
      "has_emoji                      4980       int64      Has emoji flag (0/1)\n",
      "emoji_count                    4980       int64      Count of emojis\n",
      "text_length                    4980       int64      Character count\n",
      "word_count                     4980       int64      Word count\n",
      "uppercase_ratio                4980       float64    Ratio of uppercase letters\n",
      "exclamation_count              4980       int64      Count of exclamation marks\n",
      "question_count                 4980       int64      Count of question marks\n",
      "has_exclamation                4980       int64      Has exclamation flag (0/1)\n",
      "has_question                   4980       int64      Has question flag (0/1)\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ READY FOR MODEL TRAINING!\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ RECOMMENDED USAGE:\n",
      "\n",
      "1ï¸âƒ£ FOR TRAINING (Use simple version):\n",
      "```python\n",
      "import pandas as pd\n",
      "df = pd.read_csv('perfect_training_dataset_simple.csv')\n",
      "X = df['text_clean'].values  # Features\n",
      "y = df['final_emotion_id'].values  # Labels\n",
      "```\n",
      "\n",
      "2ï¸âƒ£ FOR ANALYSIS (Use full version):\n",
      "```python\n",
      "df_full = pd.read_csv('perfect_training_dataset_full.csv')\n",
      "# All original columns are preserved for analysis\n",
      "```\n",
      "\n",
      "3ï¸âƒ£ TRAIN-TEST SPLIT:\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.2, random_state=42, stratify=y\n",
      ")\n",
      "```\n",
      "\n",
      "4ï¸âƒ£ LOAD LABEL MAPPING:\n",
      "```python\n",
      "label_mapping = pd.read_csv('emotion_label_mapping.csv')\n",
      "id_to_emotion = dict(zip(label_mapping['emotion_id'], label_mapping['emotion']))\n",
      "emotion_to_id = dict(zip(label_mapping['emotion'], label_mapping['emotion_id']))\n",
      "```\n",
      "\n",
      "ðŸ“Š DATASET VERSIONS:\n",
      "1. perfect_training_dataset_full.csv - All columns, for analysis\n",
      "2. perfect_training_dataset_simple.csv - Clean text + labels, for training\n",
      "3. emotion_label_mapping.csv - Label encoding reference\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ HAPPY MODEL TRAINING!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE DATASET CLEANING - PRESERVE ALL COLUMNS\n",
    "# Creates one perfect training dataset with cleaned data\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EMOTION DATASET CLEANING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Preserving all original columns with cleaned data\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“ STEP 1: LOADING DATASET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('merged_training_dataset.csv')\n",
    "    print(f\"âœ“ Dataset loaded successfully!\")\n",
    "    print(f\"  â€¢ Samples: {df.shape[0]:,}\")\n",
    "    print(f\"  â€¢ Features: {df.shape[1]}\")\n",
    "    \n",
    "    # Show column information\n",
    "    print(f\"\\nðŸ“‹ ORIGINAL COLUMNS:\")\n",
    "    for col in df.columns:\n",
    "        non_null = df[col].notna().sum()\n",
    "        total = len(df)\n",
    "        percentage = (non_null / total) * 100\n",
    "        print(f\"  â€¢ {col:30s}: {non_null}/{total} ({percentage:.1f}% non-null)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: File 'merged_training_dataset.csv' not found!\")\n",
    "    print(\"Please upload your dataset file to the notebook environment.\")\n",
    "    raise\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of original data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TEXT CLEANING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra quotes\n",
    "    text = text.strip('\"\\'')\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalize repeated characters\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # Normalize excessive punctuation\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract all emojis from text\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        u\"\\U00002500-\\U00002BEF\"  # Chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\U0001F9FF\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "                      \"]+\", flags=re.UNICODE)\n",
    "    return ''.join(emoji_pattern.findall(text))\n",
    "\n",
    "def expand_slang(text):\n",
    "    \"\"\"Expand common internet slang\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    slang_dict = {\n",
    "        r'\\bomg\\b': 'oh my god',\n",
    "        r'\\blol\\b': 'laughing out loud',\n",
    "        r'\\blmao\\b': 'laughing my ass off',\n",
    "        r'\\bidk\\b': 'i do not know',\n",
    "        r'\\bfr\\b': 'for real',\n",
    "        r'\\btbh\\b': 'to be honest',\n",
    "        r'\\bimo\\b': 'in my opinion',\n",
    "        r'\\bbrb\\b': 'be right back',\n",
    "        r'\\bsmh\\b': 'shaking my head',\n",
    "        r'\\basap\\b': 'as soon as possible',\n",
    "        r'\\bikr\\b': 'i know right',\n",
    "        r'\\bnvm\\b': 'never mind',\n",
    "        r'\\bwtf\\b': 'what the fuck',\n",
    "        r'\\bsrsly\\b': 'seriously',\n",
    "        r'\\bgonna\\b': 'going to',\n",
    "        r'\\bwanna\\b': 'want to',\n",
    "        r'\\bgotta\\b': 'got to',\n",
    "        r'\\bimao\\b': 'laughing my ass off',\n",
    "        r'\\bjk\\b': 'just kidding',\n",
    "        r'\\btho\\b': 'though',\n",
    "        r'\\bwyd\\b': 'what you doing',\n",
    "        r'\\bttyl\\b': 'talk to you later',\n",
    "        r'\\birl\\b': 'in real life',\n",
    "        r'\\bafaik\\b': 'as far as i know',\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in slang_dict.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_emoji_text(text):\n",
    "    \"\"\"Clean text but preserve emojis\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # First extract emojis\n",
    "    emojis = extract_emojis(text)\n",
    "    \n",
    "    # Clean the text (removing emojis temporarily)\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, ' ')\n",
    "    \n",
    "    # Clean the text\n",
    "    text = clean_text(text)\n",
    "    text = expand_slang(text)\n",
    "    \n",
    "    # Add emojis back at the end\n",
    "    if emojis:\n",
    "        text = f\"{text} {emojis}\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LABEL RESOLUTION AND CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "def clean_emotion_label(label):\n",
    "    \"\"\"Clean emotion labels\"\"\"\n",
    "    if pd.isna(label):\n",
    "        return label\n",
    "    \n",
    "    label = str(label).strip().lower()\n",
    "    \n",
    "    # Standardize common emotion labels\n",
    "    emotion_mapping = {\n",
    "        'joy': 'joy',\n",
    "        'happy': 'joy',\n",
    "        'happiness': 'joy',\n",
    "        'anger': 'anger',\n",
    "        'angry': 'anger',\n",
    "        'mad': 'anger',\n",
    "        'sadness': 'sadness',\n",
    "        'sad': 'sadness',\n",
    "        'depressed': 'sadness',\n",
    "        'fear': 'fear',\n",
    "        'scared': 'fear',\n",
    "        'afraid': 'fear',\n",
    "        'love': 'love',\n",
    "        'loving': 'love',\n",
    "        'affection': 'love',\n",
    "        'surprise': 'surprise',\n",
    "        'surprised': 'surprise',\n",
    "        'shock': 'surprise',\n",
    "        'neutral': 'neutral',\n",
    "        'none': 'neutral',\n",
    "    }\n",
    "    \n",
    "    return emotion_mapping.get(label, label)\n",
    "\n",
    "def resolve_final_emotion(row):\n",
    "    \"\"\"\n",
    "    Create final emotion label by resolving conflicts\n",
    "    Priority: hidden_emotion_label > emotion_label > emoji_emotion\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    weights = []\n",
    "    \n",
    "    # Check hidden_emotion_label\n",
    "    if pd.notna(row.get('hidden_emotion_label')):\n",
    "        hidden_label = clean_emotion_label(row['hidden_emotion_label'])\n",
    "        if hidden_label:\n",
    "            labels.append(hidden_label)\n",
    "            # Higher weight if hidden_emotion_flag is 1\n",
    "            if row.get('hidden_emotion_flag') == 1:\n",
    "                weights.append(3)\n",
    "            else:\n",
    "                weights.append(2)\n",
    "    \n",
    "    # Check emotion_label\n",
    "    if pd.notna(row.get('emotion_label')):\n",
    "        primary_label = clean_emotion_label(row['emotion_label'])\n",
    "        if primary_label:\n",
    "            labels.append(primary_label)\n",
    "            weights.append(2)\n",
    "    \n",
    "    # Check emoji_emotion\n",
    "    if pd.notna(row.get('emoji_emotion')):\n",
    "        emoji_label = clean_emotion_label(row['emoji_emotion'])\n",
    "        if emoji_label:\n",
    "            labels.append(emoji_label)\n",
    "            # Adjust weight based on emoji_text_sentiment_match\n",
    "            if row.get('emoji_text_sentiment_match') == 1:\n",
    "                weights.append(2)\n",
    "            else:\n",
    "                weights.append(1)\n",
    "    \n",
    "    # If no valid labels, return None\n",
    "    if not labels:\n",
    "        return None\n",
    "    \n",
    "    # Weighted voting\n",
    "    weighted_counter = Counter()\n",
    "    for label, weight in zip(labels, weights):\n",
    "        weighted_counter[label] += weight\n",
    "    \n",
    "    # Return most common weighted label\n",
    "    return weighted_counter.most_common(1)[0][0]\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MAIN CLEANING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ§¹ STEP 2: CLEANING DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create working copy - preserve all original columns\n",
    "df_clean = df.copy()\n",
    "original_count = len(df_clean)\n",
    "original_columns = list(df_clean.columns)\n",
    "\n",
    "print(f\"Original samples: {original_count:,}\")\n",
    "print(f\"Preserving all {len(original_columns)} columns\")\n",
    "\n",
    "# Track changes\n",
    "changes_log = []\n",
    "\n",
    "# 2.1 Clean the text column\n",
    "print(\"\\n1. Cleaning 'text' column...\")\n",
    "df_clean['text_clean'] = df_clean['text'].apply(clean_emoji_text)\n",
    "\n",
    "# Remove rows with empty text after cleaning\n",
    "initial_empty = (df_clean['text_clean'].str.strip() == '').sum()\n",
    "df_clean = df_clean[df_clean['text_clean'].str.strip() != '']\n",
    "changes_log.append(f\"Removed {initial_empty} empty texts\")\n",
    "\n",
    "# 2.2 Clean emotion label columns\n",
    "print(\"\\n2. Cleaning emotion label columns...\")\n",
    "\n",
    "# Clean individual emotion columns\n",
    "label_columns = ['hidden_emotion_label', 'emotion_label', 'emoji_emotion']\n",
    "for col in label_columns:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[f'{col}_clean'] = df_clean[col].apply(clean_emotion_label)\n",
    "        cleaned_non_null = df_clean[f'{col}_clean'].notna().sum()\n",
    "        original_non_null = df_clean[col].notna().sum()\n",
    "        print(f\"  â€¢ {col:25s}: {cleaned_non_null}/{original_non_null} cleaned\")\n",
    "\n",
    "# 2.3 Resolve final emotion label\n",
    "print(\"\\n3. Resolving final emotion label...\")\n",
    "df_clean['final_emotion'] = df_clean.apply(resolve_final_emotion, axis=1)\n",
    "\n",
    "# Remove rows where we couldn't resolve emotion\n",
    "missing_final_emotion = df_clean['final_emotion'].isna().sum()\n",
    "df_clean = df_clean[df_clean['final_emotion'].notna()]\n",
    "changes_log.append(f\"Removed {missing_final_emotion} samples with unresolved emotion\")\n",
    "\n",
    "# 2.4 Create analysis features\n",
    "print(\"\\n4. Creating analysis features...\")\n",
    "\n",
    "# Text features\n",
    "df_clean['text_length'] = df_clean['text_clean'].apply(len)\n",
    "df_clean['word_count'] = df_clean['text_clean'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Emoji features\n",
    "df_clean['emojis_extracted'] = df_clean['text'].apply(extract_emojis)\n",
    "df_clean['has_emoji'] = df_clean['emojis_extracted'].apply(lambda x: 1 if x else 0)\n",
    "df_clean['emoji_count'] = df_clean['emojis_extracted'].apply(len)\n",
    "\n",
    "# Emotional intensity features\n",
    "df_clean['uppercase_ratio'] = df_clean['text_clean'].apply(\n",
    "    lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0\n",
    ")\n",
    "df_clean['exclamation_count'] = df_clean['text_clean'].apply(lambda x: x.count('!'))\n",
    "df_clean['question_count'] = df_clean['text_clean'].apply(lambda x: x.count('?'))\n",
    "df_clean['has_exclamation'] = df_clean['exclamation_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_clean['has_question'] = df_clean['question_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# 2.5 Handle hidden_emotion_id\n",
    "print(\"\\n5. Processing hidden_emotion_id...\")\n",
    "if 'hidden_emotion_id' in df_clean.columns:\n",
    "    # Clean the ID column\n",
    "    df_clean['hidden_emotion_id'] = pd.to_numeric(df_clean['hidden_emotion_id'], errors='coerce')\n",
    "    \n",
    "    # Create mapping from ID to emotion name if not already consistent\n",
    "    id_mapping = {}\n",
    "    for _, row in df_clean.iterrows():\n",
    "        if pd.notna(row['hidden_emotion_id']) and pd.notna(row.get('hidden_emotion_label_clean')):\n",
    "            id_mapping[int(row['hidden_emotion_id'])] = row['hidden_emotion_label_clean']\n",
    "    \n",
    "    print(f\"  â€¢ Found {len(id_mapping)} unique emotion IDs\")\n",
    "\n",
    "# 2.6 Remove duplicates based on cleaned text\n",
    "print(\"\\n6. Removing duplicates...\")\n",
    "initial_duplicates = df_clean.duplicated(subset=['text_clean']).sum()\n",
    "df_clean = df_clean.drop_duplicates(subset=['text_clean'], keep='first')\n",
    "changes_log.append(f\"Removed {initial_duplicates} duplicate texts\")\n",
    "\n",
    "# 2.7 Filter very short texts\n",
    "print(\"\\n7. Filtering short texts...\")\n",
    "short_texts = (df_clean['text_length'] < 3).sum()\n",
    "df_clean = df_clean[df_clean['text_length'] >= 3]\n",
    "changes_log.append(f\"Removed {short_texts} very short texts (<3 chars)\")\n",
    "\n",
    "# 2.8 Encode final emotion labels\n",
    "print(\"\\n8. Encoding final emotion labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['final_emotion_id'] = label_encoder.fit_transform(df_clean['final_emotion'])\n",
    "\n",
    "# ============================================================================\n",
    "# 5. QUALITY FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ” STEP 3: QUALITY FILTERING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3.1 Remove emotion classes with too few samples\n",
    "emotion_counts = df_clean['final_emotion'].value_counts()\n",
    "min_samples_per_class = 5\n",
    "rare_emotions = emotion_counts[emotion_counts < min_samples_per_class].index.tolist()\n",
    "\n",
    "if rare_emotions:\n",
    "    rare_count = emotion_counts[emotion_counts < min_samples_per_class].sum()\n",
    "    df_clean = df_clean[~df_clean['final_emotion'].isin(rare_emotions)]\n",
    "    changes_log.append(f\"Removed {len(rare_emotions)} rare emotions ({rare_count} samples) with <{min_samples_per_class} samples\")\n",
    "\n",
    "# 3.2 Filter based on confidence\n",
    "print(\"\\n9. Calculating confidence scores...\")\n",
    "\n",
    "def calculate_confidence(row):\n",
    "    \"\"\"Calculate confidence score for each sample\"\"\"\n",
    "    confidence = 1.0  # Base confidence\n",
    "    \n",
    "    # Check label agreement\n",
    "    labels = []\n",
    "    if pd.notna(row.get('hidden_emotion_label_clean')):\n",
    "        labels.append(row['hidden_emotion_label_clean'])\n",
    "    if pd.notna(row.get('emotion_label_clean')):\n",
    "        labels.append(row['emotion_label_clean'])\n",
    "    if pd.notna(row.get('emoji_emotion_clean')):\n",
    "        labels.append(row['emoji_emotion_clean'])\n",
    "    \n",
    "    if len(set(labels)) == 1 and len(labels) > 1:  # All labels agree\n",
    "        confidence *= 1.5\n",
    "    elif len(set(labels)) == 2 and len(labels) >= 2:  # 2 out of 3 agree\n",
    "        confidence *= 1.2\n",
    "    \n",
    "    # Adjust for emoji-text match\n",
    "    if 'emoji_text_sentiment_match' in row and pd.notna(row['emoji_text_sentiment_match']):\n",
    "        if row['emoji_text_sentiment_match'] == 1:\n",
    "            confidence *= 1.1\n",
    "    \n",
    "    # Adjust for text length (longer texts are more reliable)\n",
    "    if row['text_length'] > 20:\n",
    "        confidence *= 1.1\n",
    "    elif row['text_length'] < 10:\n",
    "        confidence *= 0.9\n",
    "    \n",
    "    return min(confidence, 2.0)  # Cap at 2.0\n",
    "\n",
    "df_clean['confidence_score'] = df_clean.apply(calculate_confidence, axis=1)\n",
    "\n",
    "# Filter low confidence samples\n",
    "confidence_threshold = 0.8\n",
    "low_confidence = (df_clean['confidence_score'] < confidence_threshold).sum()\n",
    "df_clean = df_clean[df_clean['confidence_score'] >= confidence_threshold]\n",
    "changes_log.append(f\"Removed {low_confidence} low-confidence samples (confidence < {confidence_threshold})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. CLASS BALANCING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nâš–ï¸  STEP 4: BALANCING CLASSES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "emotion_counts = df_clean['final_emotion'].value_counts()\n",
    "max_count = emotion_counts.max()\n",
    "min_count = emotion_counts.min()\n",
    "imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "\n",
    "print(f\"Current class distribution:\")\n",
    "for emotion, count in emotion_counts.items():\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  â€¢ {emotion:12s}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "if imbalance_ratio > 4 and min_count > 0:\n",
    "    print(\"\\nâš ï¸  Class imbalance detected! Applying balancing...\")\n",
    "    \n",
    "    # Use median as target, but keep at least min_count\n",
    "    target_samples = max(int(emotion_counts.median()), min_count * 2)\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for emotion in df_clean['final_emotion'].unique():\n",
    "        emotion_df = df_clean[df_clean['final_emotion'] == emotion]\n",
    "        current_count = len(emotion_df)\n",
    "        \n",
    "        if current_count > target_samples:\n",
    "            # Undersample: keep high-confidence samples\n",
    "            emotion_df = emotion_df.sort_values('confidence_score', ascending=False)\n",
    "            emotion_df = emotion_df.head(target_samples)\n",
    "            print(f\"  â†“ {emotion:12s}: {current_count:4d} â†’ {len(emotion_df):4d} (undersampled)\")\n",
    "        elif current_count < target_samples * 0.7:\n",
    "            # Oversample only if significantly smaller\n",
    "            repeats = target_samples // current_count\n",
    "            remainder = target_samples % current_count\n",
    "            \n",
    "            oversampled = pd.concat([emotion_df] * repeats, ignore_index=True)\n",
    "            if remainder > 0:\n",
    "                additional = emotion_df.sample(n=remainder, random_state=42, replace=True)\n",
    "                oversampled = pd.concat([oversampled, additional], ignore_index=True)\n",
    "            \n",
    "            emotion_df = oversampled\n",
    "            print(f\"  â†‘ {emotion:12s}: {current_count:4d} â†’ {len(emotion_df):4d} (oversampled)\")\n",
    "        else:\n",
    "            print(f\"  â€¢ {emotion:12s}: {current_count:4d} (kept as is)\")\n",
    "        \n",
    "        balanced_dfs.append(emotion_df)\n",
    "    \n",
    "    df_clean = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle\n",
    "    df_clean = df_clean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nâœ“ Balancing complete!\")\n",
    "else:\n",
    "    print(\"âœ“ Class distribution is acceptable\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. FINAL DATASET ORGANIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š STEP 5: ORGANIZING FINAL DATASET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Organize columns in a logical order\n",
    "column_groups = {\n",
    "    'IDENTIFIERS': ['text', 'text_clean'],  # Original and cleaned text\n",
    "    'EMOTION_LABELS': [\n",
    "        'hidden_emotion_label', 'hidden_emotion_label_clean',\n",
    "        'emotion_label', 'emotion_label_clean',\n",
    "        'emoji_emotion', 'emoji_emotion_clean',\n",
    "        'final_emotion', 'final_emotion_id'\n",
    "    ],\n",
    "    'METADATA': [\n",
    "        'hidden_emotion_flag', 'emoji_text_sentiment_match',\n",
    "        'hidden_emotion_id', 'confidence_score'\n",
    "    ],\n",
    "    'EMOJI_INFO': [\n",
    "        'primary_emoji', 'emojis_extracted',\n",
    "        'has_emoji', 'emoji_count'\n",
    "    ],\n",
    "    'TEXT_FEATURES': [\n",
    "        'text_length', 'word_count',\n",
    "        'uppercase_ratio', 'exclamation_count', 'question_count',\n",
    "        'has_exclamation', 'has_question'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create final column order\n",
    "final_columns = []\n",
    "for group_name, group_columns in column_groups.items():\n",
    "    # Only add columns that exist in the dataframe\n",
    "    existing_columns = [col for col in group_columns if col in df_clean.columns]\n",
    "    if existing_columns:\n",
    "        final_columns.extend(existing_columns)\n",
    "\n",
    "# Add any remaining columns not in the groups\n",
    "all_columns = list(df_clean.columns)\n",
    "remaining_columns = [col for col in all_columns if col not in final_columns]\n",
    "if remaining_columns:\n",
    "    final_columns.extend(remaining_columns)\n",
    "\n",
    "# Create final dataset\n",
    "df_final = df_clean[final_columns].copy()\n",
    "\n",
    "print(f\"Final dataset has {len(df_final):,} samples and {len(df_final.columns)} columns\")\n",
    "print(f\"\\nColumn groups in final dataset:\")\n",
    "for group_name, group_columns in column_groups.items():\n",
    "    existing = [col for col in group_columns if col in df_final.columns]\n",
    "    if existing:\n",
    "        print(f\"  â€¢ {group_name}: {len(existing)} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. QUALITY ASSURANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nâœ… STEP 6: QUALITY ASSURANCE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "checks_passed = 0\n",
    "checks_total = 6\n",
    "\n",
    "# Check 1: No missing values in critical columns\n",
    "critical_cols = ['text_clean', 'final_emotion', 'final_emotion_id']\n",
    "missing_critical = df_final[critical_cols].isnull().sum().sum()\n",
    "if missing_critical == 0:\n",
    "    print(f\"âœ“ No missing values in critical columns\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âœ— Missing values in critical columns: {missing_critical}\")\n",
    "\n",
    "# Check 2: No empty texts\n",
    "empty_texts = (df_final['text_clean'].str.strip() == '').sum()\n",
    "if empty_texts == 0:\n",
    "    print(f\"âœ“ No empty texts\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âœ— Empty texts: {empty_texts}\")\n",
    "\n",
    "# Check 3: No duplicates\n",
    "duplicates = df_final.duplicated(subset=['text_clean']).sum()\n",
    "if duplicates == 0:\n",
    "    print(f\"âœ“ No duplicate texts\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âœ— Duplicate texts: {duplicates}\")\n",
    "\n",
    "# Check 4: Minimum text length\n",
    "min_length = df_final['text_length'].min()\n",
    "if min_length >= 3:\n",
    "    print(f\"âœ“ Minimum text length: {min_length} (OK)\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âœ— Text too short: {min_length}\")\n",
    "\n",
    "# Check 5: Class size\n",
    "class_sizes = df_final['final_emotion'].value_counts()\n",
    "min_class = class_sizes.min()\n",
    "if min_class >= 5:\n",
    "    print(f\"âœ“ Minimum class size: {min_class} (OK)\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âœ— Class too small: {min_class}\")\n",
    "\n",
    "# Check 6: Valid emotion IDs\n",
    "invalid_ids = df_final[~df_final['final_emotion'].isin(label_encoder.classes_)].shape[0]\n",
    "if invalid_ids == 0:\n",
    "    print(f\"âœ“ All emotion labels are valid\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"âœ— Invalid emotion labels: {invalid_ids}\")\n",
    "\n",
    "print(f\"\\nQuality checks: {checks_passed}/{checks_total} passed\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. SAVE DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ’¾ STEP 7: SAVING DATASETS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the main dataset\n",
    "output_file = 'perfect_training_dataset_full.csv'\n",
    "df_final.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"âœ“ Perfect training dataset saved: {output_file}\")\n",
    "print(f\"  â€¢ Samples: {len(df_final):,}\")\n",
    "print(f\"  â€¢ Columns: {len(df_final.columns)}\")\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = pd.DataFrame({\n",
    "    'emotion': label_encoder.classes_,\n",
    "    'emotion_id': label_encoder.transform(label_encoder.classes_),\n",
    "    'count': [len(df_final[df_final['final_emotion'] == emotion]) for emotion in label_encoder.classes_]\n",
    "})\n",
    "label_mapping.to_csv('emotion_label_mapping.csv', index=False)\n",
    "print(f\"âœ“ Label mapping saved: emotion_label_mapping.csv\")\n",
    "\n",
    "# Save a simplified version for quick use\n",
    "simple_columns = ['text_clean', 'final_emotion', 'final_emotion_id', 'confidence_score']\n",
    "df_simple = df_final[simple_columns].copy()\n",
    "df_simple.to_csv('perfect_training_dataset_simple.csv', index=False)\n",
    "print(f\"âœ“ Simplified dataset saved: perfect_training_dataset_simple.csv\")\n",
    "\n",
    "# Save cleaning log\n",
    "log_df = pd.DataFrame({\n",
    "    'step': ['Original'] + [f'Step {i+1}' for i in range(len(changes_log))],\n",
    "    'description': ['Initial dataset'] + changes_log,\n",
    "    'samples': [original_count] + [len(df_final)] * len(changes_log)\n",
    "})\n",
    "log_df.to_csv('cleaning_log.csv', index=False)\n",
    "print(f\"âœ“ Cleaning log saved: cleaning_log.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“ˆ FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET STATISTICS:\")\n",
    "print(f\"  â€¢ Total samples: {len(df_final):,}\")\n",
    "print(f\"  â€¢ Unique emotions: {df_final['final_emotion'].nunique()}\")\n",
    "print(f\"  â€¢ Average text length: {df_final['text_length'].mean():.1f} chars\")\n",
    "print(f\"  â€¢ Average word count: {df_final['word_count'].mean():.1f} words\")\n",
    "print(f\"  â€¢ Samples with emojis: {df_final['has_emoji'].sum():,} ({df_final['has_emoji'].mean()*100:.1f}%)\")\n",
    "print(f\"  â€¢ Average confidence: {df_final['confidence_score'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ­ EMOTION DISTRIBUTION:\")\n",
    "emotion_stats = df_final['final_emotion'].value_counts().sort_values(ascending=False)\n",
    "for emotion, count in emotion_stats.items():\n",
    "    percentage = (count / len(df_final)) * 100\n",
    "    print(f\"  â€¢ {emotion:12s}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ SAVED FILES:\")\n",
    "print(f\"  1. {output_file} - Full dataset with all columns ({len(df_final):,} samples)\")\n",
    "print(f\"  2. perfect_training_dataset_simple.csv - Simplified version\")\n",
    "print(f\"  3. emotion_label_mapping.csv - Emotion to ID mapping\")\n",
    "print(f\"  4. cleaning_log.csv - Cleaning process log\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… PIPELINE COMPLETE! Perfect dataset is ready for training!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# 11. SAMPLE PREVIEW\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ‘€ SAMPLE OF FINAL DATASET:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "sample_size = min(3, len(df_final))\n",
    "for i in range(sample_size):\n",
    "    sample = df_final.iloc[i]\n",
    "    print(f\"\\nðŸ“ Sample {i+1}:\")\n",
    "    \n",
    "    # Text preview\n",
    "    text_preview = sample['text_clean']\n",
    "    if len(text_preview) > 60:\n",
    "        text_preview = text_preview[:57] + \"...\"\n",
    "    print(f\"  Text: {text_preview}\")\n",
    "    \n",
    "    # Emotion info\n",
    "    print(f\"  Final Emotion: {sample['final_emotion']} (ID: {sample['final_emotion_id']})\")\n",
    "    \n",
    "    # Original labels (if available)\n",
    "    if 'hidden_emotion_label_clean' in sample and pd.notna(sample['hidden_emotion_label_clean']):\n",
    "        print(f\"  Hidden Emotion: {sample['hidden_emotion_label_clean']}\")\n",
    "    if 'emotion_label_clean' in sample and pd.notna(sample['emotion_label_clean']):\n",
    "        print(f\"  Primary Emotion: {sample['emotion_label_clean']}\")\n",
    "    if 'emoji_emotion_clean' in sample and pd.notna(sample['emoji_emotion_clean']):\n",
    "        print(f\"  Emoji Emotion: {sample['emoji_emotion_clean']}\")\n",
    "    \n",
    "    # Metadata\n",
    "    print(f\"  Confidence: {sample.get('confidence_score', 'N/A'):.2f}\")\n",
    "    print(f\"  Length: {sample['text_length']} chars, {sample['word_count']} words\")\n",
    "    \n",
    "    if sample.get('has_emoji', 0) == 1:\n",
    "        print(f\"  Emojis: {sample.get('emojis_extracted', '')}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# 12. DATASET ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET ANALYSIS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Show column information\n",
    "print(\"\\nðŸ“‹ COLUMN INFORMATION:\")\n",
    "print(f\"{'Column':<30} {'Non-Null':<10} {'Dtype':<10} {'Description'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "column_descriptions = {\n",
    "    'text': 'Original text',\n",
    "    'text_clean': 'Cleaned text (main feature)',\n",
    "    'hidden_emotion_label': 'Original hidden emotion label',\n",
    "    'hidden_emotion_label_clean': 'Cleaned hidden emotion label',\n",
    "    'emotion_label': 'Original primary emotion label',\n",
    "    'emotion_label_clean': 'Cleaned primary emotion label',\n",
    "    'emoji_emotion': 'Original emoji emotion',\n",
    "    'emoji_emotion_clean': 'Cleaned emoji emotion',\n",
    "    'final_emotion': 'Resolved final emotion label',\n",
    "    'final_emotion_id': 'Encoded emotion ID',\n",
    "    'hidden_emotion_flag': 'Flag for hidden emotion (0/1)',\n",
    "    'emoji_text_sentiment_match': 'Emoji-text match flag (0/1)',\n",
    "    'primary_emoji': 'Primary emoji from text',\n",
    "    'hidden_emotion_id': 'Hidden emotion ID',\n",
    "    'confidence_score': 'Confidence score (0-2)',\n",
    "    'emojis_extracted': 'All emojis extracted from text',\n",
    "    'has_emoji': 'Has emoji flag (0/1)',\n",
    "    'emoji_count': 'Count of emojis',\n",
    "    'text_length': 'Character count',\n",
    "    'word_count': 'Word count',\n",
    "    'uppercase_ratio': 'Ratio of uppercase letters',\n",
    "    'exclamation_count': 'Count of exclamation marks',\n",
    "    'question_count': 'Count of question marks',\n",
    "    'has_exclamation': 'Has exclamation flag (0/1)',\n",
    "    'has_question': 'Has question flag (0/1)',\n",
    "}\n",
    "\n",
    "for col in df_final.columns:\n",
    "    non_null = df_final[col].notna().sum()\n",
    "    total = len(df_final)\n",
    "    dtype = str(df_final[col].dtype)\n",
    "    description = column_descriptions.get(col, 'Additional feature')\n",
    "    print(f\"{col:<30} {non_null:<10} {dtype:<10} {description}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 13. READY FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸš€ READY FOR MODEL TRAINING!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸŽ¯ RECOMMENDED USAGE:\")\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ FOR TRAINING (Use simple version):\")\n",
    "print(\"```python\")\n",
    "print(\"import pandas as pd\")\n",
    "print(\"df = pd.read_csv('perfect_training_dataset_simple.csv')\")\n",
    "print(\"X = df['text_clean'].values  # Features\")\n",
    "print(\"y = df['final_emotion_id'].values  # Labels\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ FOR ANALYSIS (Use full version):\")\n",
    "print(\"```python\")\n",
    "print(\"df_full = pd.read_csv('perfect_training_dataset_full.csv')\")\n",
    "print(\"# All original columns are preserved for analysis\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ TRAIN-TEST SPLIT:\")\n",
    "print(\"```python\")\n",
    "print(\"from sklearn.model_selection import train_test_split\")\n",
    "print(\"X_train, X_test, y_train, y_test = train_test_split(\")\n",
    "print(\"    X, y, test_size=0.2, random_state=42, stratify=y\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ LOAD LABEL MAPPING:\")\n",
    "print(\"```python\")\n",
    "print(\"label_mapping = pd.read_csv('emotion_label_mapping.csv')\")\n",
    "print(\"id_to_emotion = dict(zip(label_mapping['emotion_id'], label_mapping['emotion']))\")\n",
    "print(\"emotion_to_id = dict(zip(label_mapping['emotion'], label_mapping['emotion_id']))\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET VERSIONS:\")\n",
    "print(\"1. perfect_training_dataset_full.csv - All columns, for analysis\")\n",
    "print(\"2. perfect_training_dataset_simple.csv - Clean text + labels, for training\")\n",
    "print(\"3. emotion_label_mapping.csv - Label encoding reference\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸŽ‰ HAPPY MODEL TRAINING!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ca6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6b63a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EMOTION DATASET CLEANING PIPELINE\n",
      "======================================================================\n",
      "Enhanced cleaning: removes short texts, HTML tags, links\n",
      "\n",
      "ðŸ“ STEP 1: LOADING DATASET\n",
      "--------------------------------------------------\n",
      "âœ“ Dataset loaded successfully!\n",
      "  â€¢ Samples: 6,755\n",
      "  â€¢ Features: 8\n",
      "\n",
      "Sample of original data:\n",
      "                                                text hidden_emotion_label  \\\n",
      "0  \"Woah did you draw that?\"\\nNo, I Printed it. O...                  joy   \n",
      "1                       I CANT WITH THESE COMMENTSðŸ˜­ðŸ™                anger   \n",
      "\n",
      "   hidden_emotion_flag  emoji_text_sentiment_match emotion_label  \\\n",
      "0                    1                           0         anger   \n",
      "1                    1                           0       sadness   \n",
      "\n",
      "  primary_emoji emoji_emotion  hidden_emotion_id  \n",
      "0             ðŸ‘¹         anger                  2  \n",
      "1             ðŸ˜­       sadness                  0  \n",
      "\n",
      "ðŸ§¹ STEP 2: CLEANING DATA\n",
      "--------------------------------------------------\n",
      "Original samples: 6,755\n",
      "\n",
      "1. Enhanced text cleaning...\n",
      "   â€¢ Removing HTML tags\n",
      "   â€¢ Removing URLs/links\n",
      "   â€¢ Removing problematic patterns\n",
      "   â€¢ Expanding slang\n",
      "   â€¢ Preserving meaningful emojis\n",
      "\n",
      "   Cleaning examples:\n",
      "   1. Original: \"Woah did you draw that?\"\n",
      "No, I Printed it. OF COU...\n",
      "      Cleaned: \"Woah did you draw that?\" No, I Printed it. OF COU...\n",
      "   2. Original: I CANT WITH THESE COMMENTSðŸ˜­ðŸ™...\n",
      "      Cleaned: I CANT WITH THESE COMMENTS ðŸ˜­ðŸ™...\n",
      "   3. Original: \" Oh sorry, were you sleeping? \" No, I'm summoning...\n",
      "      Cleaned: \" Oh sorry, were you sleeping? \" No, I'm summoning...\n",
      "\n",
      "   âœ“ Removed 7 empty/invalid texts\n",
      "\n",
      "2. Cleaning emotion labels in-place...\n",
      "   â€¢ Cleaned hidden_emotion_label\n",
      "   â€¢ Cleaned emotion_label\n",
      "   â€¢ Cleaned emoji_emotion\n",
      "\n",
      "3. Determining final emotion...\n",
      "   âœ“ Removed 0 rows without final emotion\n",
      "\n",
      "4. Filtering short and meaningless texts...\n",
      "   âœ“ Removed 65 short/meaningless texts\n",
      "\n",
      "5. Removing duplicates...\n",
      "   âœ“ Removed 834 duplicate texts\n",
      "\n",
      "6. Filtering rare emotions...\n",
      "   âœ“ No rare emotions to remove\n",
      "\n",
      "âœ“ Final dataset size: 5,849 samples\n",
      "\n",
      "ðŸ”¢ STEP 3: CREATING EMOTION IDS\n",
      "--------------------------------------------------\n",
      "âœ“ Created emotion IDs for 6 emotions\n",
      "\n",
      "ðŸ“Š STEP 4: ORGANIZING FINAL DATASET\n",
      "--------------------------------------------------\n",
      "Final dataset structure:\n",
      "  â€¢ Total columns: 11\n",
      "  â€¢ New columns added: 3\n",
      "  â€¢ Original columns preserved: 8\n",
      "\n",
      "New columns:\n",
      "  â€¢ text_clean\n",
      "  â€¢ final_emotion\n",
      "  â€¢ emotion_id\n",
      "\n",
      "âœ… STEP 5: QUALITY CHECKS\n",
      "--------------------------------------------------\n",
      "âœ“ HTML tags completely removed\n",
      "âœ— Found 1 texts with URLs\n",
      "âœ“ No very short texts (<8 chars)\n",
      "âœ“ All texts have meaningful content\n",
      "\n",
      "Quality checks: 4/4 passed\n",
      "\n",
      "ðŸ’¾ STEP 6: SAVING DATASETS\n",
      "--------------------------------------------------\n",
      "âœ“ Main dataset saved: cleaned_emotion_dataset.csv\n",
      "  â€¢ Samples: 5,849\n",
      "  â€¢ Columns: 11\n",
      "âœ“ Label mapping saved: emotion_label_mapping.csv\n",
      "âœ“ Training dataset saved: training_dataset.csv\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š DATASET STATISTICS:\n",
      "  â€¢ Original samples: 6,755\n",
      "  â€¢ Final samples: 5,849\n",
      "  â€¢ Removed: 906 samples (13.4%)\n",
      "  â€¢ Unique emotions: 6\n",
      "\n",
      "ðŸ§¹ CLEANING SUMMARY:\n",
      "  â€¢ Empty texts removed: 7\n",
      "  â€¢ No emotion removed: 0\n",
      "  â€¢ Short/meaningless texts removed: 65\n",
      "  â€¢ Duplicates removed: 834\n",
      "\n",
      "ðŸŽ­ EMOTION DISTRIBUTION:\n",
      "  â€¢ joy         :  2741 samples ( 46.9%)\n",
      "  â€¢ sadness     :  1180 samples ( 20.2%)\n",
      "  â€¢ anger       :   674 samples ( 11.5%)\n",
      "  â€¢ love        :   518 samples (  8.9%)\n",
      "  â€¢ fear        :   461 samples (  7.9%)\n",
      "  â€¢ surprise    :   275 samples (  4.7%)\n",
      "\n",
      "ðŸ“ TEXT LENGTH ANALYSIS:\n",
      "  â€¢ Min length: 8 chars\n",
      "  â€¢ Average length: 71.6 chars\n",
      "  â€¢ Max length: 198 chars\n",
      "  â€¢ Short texts (8-20 chars): 189\n",
      "  â€¢ Medium texts (20-100 chars): 4,772\n",
      "  â€¢ Long texts (>100 chars): 888\n",
      "\n",
      "ðŸ˜€ EMOJI PRESERVATION:\n",
      "  â€¢ Samples with emojis: 5,849 (100.0%)\n",
      "\n",
      "ðŸ“ SAVED FILES:\n",
      "  1. cleaned_emotion_dataset.csv - Full cleaned dataset (5,849 samples)\n",
      "  2. training_dataset.csv - Training dataset (text_clean + emotions)\n",
      "  3. emotion_label_mapping.csv - Emotion ID mapping\n",
      "\n",
      "======================================================================\n",
      "âœ… ENHANCED CLEANING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸ‘€ SAMPLE OF CLEANED DATA:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ“ BEFORE/AFTER CLEANING EXAMPLES:\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ“ REGULAR SAMPLES:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "  Cleaned text: \"Woah did you draw that?\" No, I Printed it. OF COURSE I DREW IT ðŸ‘¹\n",
      "  Final emotion: joy (ID: 2)\n",
      "  Hidden emotion: joy\n",
      "  Emotion label: anger\n",
      "  Emojis preserved: ðŸ‘¹\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "  Cleaned text: I CANT WITH THESE COMMENTS ðŸ˜­ðŸ™\n",
      "  Final emotion: sadness (ID: 4)\n",
      "  Hidden emotion: anger\n",
      "  Emotion label: sadness\n",
      "  Emojis preserved: ðŸ˜­ðŸ™\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸš« TYPES OF CONTENT REMOVED:\n",
      "----------------------------------------------------------------------\n",
      "1. HTML TAGS:\n",
      "   â€¢ <div>content</div> â†’ 'content'\n",
      "   â€¢ <br> â†’ removed\n",
      "   â€¢ &amp; â†’ 'and'\n",
      "\n",
      "2. URLs/LINKS:\n",
      "   â€¢ http://example.com â†’ removed\n",
      "   â€¢ www.google.com â†’ removed\n",
      "   â€¢ bit.ly/abc123 â†’ removed\n",
      "\n",
      "3. SHORT TEXTS:\n",
      "   â€¢ 'lol' â†’ removed\n",
      "   â€¢ 'k' â†’ removed\n",
      "   â€¢ 'ok' â†’ removed if no context\n",
      "\n",
      "4. PROBLEMATIC PATTERNS:\n",
      "   â€¢ '@Leaâ˜ï¸ðŸ’€' â†’ removed\n",
      "   â€¢ '@user' â†’ removed\n",
      "   â€¢ 'ðŸ˜‚' â†’ removed (if standalone)\n",
      "\n",
      "âœ… TYPES OF CONTENT KEPT:\n",
      "   â€¢ 'I'm so happy! ðŸ˜Š' â†’ kept\n",
      "   â€¢ 'Feeling anxious today :(' â†’ kept\n",
      "   â€¢ 'This made me laugh ðŸ˜‚ so much!' â†’ kept\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE DATASET CLEANING - MINIMAL COLUMNS\n",
    "# Enhanced to remove short texts, HTML tags, and links\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EMOTION DATASET CLEANING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Enhanced cleaning: removes short texts, HTML tags, links\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“ STEP 1: LOADING DATASET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('merged_training_dataset.csv')\n",
    "    print(f\"âœ“ Dataset loaded successfully!\")\n",
    "    print(f\"  â€¢ Samples: {df.shape[0]:,}\")\n",
    "    print(f\"  â€¢ Features: {df.shape[1]}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: File 'merged_training_dataset.csv' not found!\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nSample of original data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ENHANCED TEXT CLEANING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract all emojis from text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        u\"\\U00002500-\\U00002BEF\"  # Chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\U0001F9FF\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "                      \"]+\", flags=re.UNICODE)\n",
    "    return ''.join(emoji_pattern.findall(text))\n",
    "\n",
    "def clean_text_enhanced(text):\n",
    "    \"\"\"\n",
    "    Enhanced text cleaning that:\n",
    "    1. Removes HTML tags completely\n",
    "    2. Removes URLs/links completely\n",
    "    3. Removes problematic patterns\n",
    "    4. Preserves meaningful emojis\n",
    "    5. Expands internet slang\n",
    "    6. Removes very short meaningless texts\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    original_text = text.strip()\n",
    "    \n",
    "    # ============================================\n",
    "    # 1. REMOVE PROBLEMATIC PATTERNS FIRST\n",
    "    # ============================================\n",
    "    \n",
    "    # Remove patterns like @Leaâ˜ï¸ðŸ’€, @userðŸ˜‚, etc.\n",
    "    problematic_patterns = [\n",
    "        r'^@\\w+[â˜ï¸ðŸ’€ðŸ¤£ðŸ˜‚ðŸ˜­ðŸ˜ŠðŸ¥°ðŸ˜¡ðŸ˜¢ðŸ˜”ðŸ˜žðŸ˜ŸðŸ˜©ðŸ¥ºðŸ˜¨ðŸ˜°ðŸ˜±ðŸ˜–ðŸ˜³â¤ï¸ðŸ’•ðŸ’–ðŸ’—ðŸ’“ðŸ’žðŸ’˜ðŸ’ðŸ˜˜ðŸ’‹ðŸ˜²ðŸ˜¯ðŸ˜®ðŸ˜ðŸ˜‘ðŸ˜¶]+$',\n",
    "        r'^[â˜ï¸ðŸ’€ðŸ¤£ðŸ˜‚ðŸ˜­ðŸ˜ŠðŸ¥°ðŸ˜¡ðŸ˜¢ðŸ˜”ðŸ˜žðŸ˜ŸðŸ˜©ðŸ¥ºðŸ˜¨ðŸ˜°ðŸ˜±ðŸ˜–ðŸ˜³â¤ï¸ðŸ’•ðŸ’–ðŸ’—ðŸ’“ðŸ’žðŸ’˜ðŸ’ðŸ˜˜ðŸ’‹ðŸ˜²ðŸ˜¯ðŸ˜®ðŸ˜ðŸ˜‘ðŸ˜¶]+$',\n",
    "        r'^@\\w+$',  # Just a mention\n",
    "        r'^[\\s\\W]+$',  # Only special characters/whitespace\n",
    "    ]\n",
    "    \n",
    "    for pattern in problematic_patterns:\n",
    "        if re.match(pattern, original_text):\n",
    "            return \"\"  # Mark for removal\n",
    "    \n",
    "    # ============================================\n",
    "    # 2. REMOVE HTML TAGS COMPLETELY\n",
    "    # ============================================\n",
    "    \n",
    "    # Remove all HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', original_text)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    html_entities = {\n",
    "        '&amp;': ' and ',\n",
    "        '&lt;': ' ',\n",
    "        '&gt;': ' ',\n",
    "        '&quot;': '\"',\n",
    "        '&#039;': \"'\",\n",
    "        '&#39;': \"'\",\n",
    "        '&nbsp;': ' ',\n",
    "        '&rsquo;': \"'\",\n",
    "        '&lsquo;': \"'\",\n",
    "        '&ldquo;': '\"',\n",
    "        '&rdquo;': '\"',\n",
    "    }\n",
    "    \n",
    "    for entity, replacement in html_entities.items():\n",
    "        text = text.replace(entity, replacement)\n",
    "    \n",
    "    # Remove any remaining &#xxx; patterns\n",
    "    text = re.sub(r'&#\\d+;', ' ', text)\n",
    "    \n",
    "    # ============================================\n",
    "    # 3. REMOVE URLs/LINKS COMPLETELY\n",
    "    # ============================================\n",
    "    \n",
    "    # Remove all types of URLs\n",
    "    url_patterns = [\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "        r'www\\.[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "        r'[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/[a-zA-Z0-9./?=&_-]*',\n",
    "        r'bit\\.ly/[a-zA-Z0-9]+',\n",
    "        r't\\.co/[a-zA-Z0-9]+',\n",
    "        r'goo\\.gl/[a-zA-Z0-9]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in url_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # ============================================\n",
    "    # 4. EXTRACT AND PRESERVE EMOJIS\n",
    "    # ============================================\n",
    "    \n",
    "    emojis = extract_emojis(text)\n",
    "    \n",
    "    # Remove emojis temporarily for text processing\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, ' ')\n",
    "    \n",
    "    # ============================================\n",
    "    # 5. CLEAN TEXT CONTENT\n",
    "    # ============================================\n",
    "    \n",
    "    # Remove all mentions completely\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtag symbols but keep text\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Expand internet slang\n",
    "    slang_dict = {\n",
    "        r'\\bu\\b': 'you',\n",
    "        r'\\br\\b': 'are',\n",
    "        r'\\bur\\b': 'your',\n",
    "        r'\\b2\\b': 'to',\n",
    "        r'\\b4\\b': 'for',\n",
    "        r'\\bw\\b': 'with',\n",
    "        r'\\bw/o\\b': 'without',\n",
    "        r'\\bb4\\b': 'before',\n",
    "        r'\\bcuz\\b': 'because',\n",
    "        r'\\bppl\\b': 'people',\n",
    "        r'\\btho\\b': 'though',\n",
    "        r'\\bpls\\b': 'please',\n",
    "        r'\\bplz\\b': 'please',\n",
    "        r'\\bsry\\b': 'sorry',\n",
    "        r'\\bsup\\b': 'what is up',\n",
    "        r'\\bwyd\\b': 'what are you doing',\n",
    "        r'\\bomg\\b': 'oh my god',\n",
    "        r'\\blol\\b': 'laughing out loud',\n",
    "        r'\\blmao\\b': 'laughing my ass off',\n",
    "        r'\\bimao\\b': 'laughing my ass off',\n",
    "        r'\\bidk\\b': 'i do not know',\n",
    "        r'\\btbh\\b': 'to be honest',\n",
    "        r'\\bnvm\\b': 'never mind',\n",
    "        r'\\bjk\\b': 'just kidding',\n",
    "        r'\\bfr\\b': 'for real',\n",
    "        r'\\by\\b': 'why',\n",
    "        r'\\bn\\b': 'and',\n",
    "        r'\\brn\\b': 'right now',\n",
    "        r'\\bttyl\\b': 'talk to you later',\n",
    "        r'\\bbrb\\b': 'be right back',\n",
    "        r'\\bgtg\\b': 'got to go',\n",
    "        r'\\bio\\b': 'input output',\n",
    "        r'\\baka\\b': 'also known as',\n",
    "        r'\\basap\\b': 'as soon as possible',\n",
    "        r'\\bimo\\b': 'in my opinion',\n",
    "        r'\\bfomo\\b': 'fear of missing out',\n",
    "        r'\\byolo\\b': 'you only live once',\n",
    "        r'\\bwtf\\b': 'what the heck',\n",
    "        r'\\bsmh\\b': 'shaking my head',\n",
    "        r'\\bafk\\b': 'away from keyboard',\n",
    "        r'\\birl\\b': 'in real life',\n",
    "        r'\\bofc\\b': 'of course',\n",
    "        r'\\bfyi\\b': 'for your information',\n",
    "        r'\\bg2g\\b': 'got to go',\n",
    "        r'\\bg2k\\b': 'good to know',\n",
    "        r'\\bhm\\b': 'hmm',\n",
    "        r'\\bnbd\\b': 'no big deal',\n",
    "        r'\\botp\\b': 'on the phone',\n",
    "        r'\\brofl\\b': 'rolling on floor laughing',\n",
    "        r'\\btgif\\b': 'thank god it is friday',\n",
    "        r'\\btmi\\b': 'too much information',\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in slang_dict.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # ============================================\n",
    "    # 6. CLEAN PUNCTUATION AND FORMATTING\n",
    "    # ============================================\n",
    "    \n",
    "    # Remove excessive punctuation\n",
    "    text = re.sub(r'([!?.]){3,}', r'\\1', text)\n",
    "    \n",
    "    # Remove special characters except basic punctuation and letters\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"-]', ' ', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # ============================================\n",
    "    # 7. ADD EMOJIS BACK\n",
    "    # ============================================\n",
    "    \n",
    "    # Add emojis back if we have meaningful text\n",
    "    if emojis and len(text.strip()) > 0:\n",
    "        text = f\"{text} {emojis}\".strip()\n",
    "    elif emojis and len(text.strip()) == 0:\n",
    "        # If only emojis remain, create meaningful text\n",
    "        emoji_to_text = {\n",
    "            'ðŸ˜‚': 'laughing', 'ðŸ¤£': 'laughing hard', 'ðŸ˜Š': 'smiling',\n",
    "            'ðŸ˜­': 'crying', 'ðŸ˜¢': 'tearful', 'ðŸ˜”': 'sad',\n",
    "            'ðŸ˜¡': 'angry', 'â¤ï¸': 'love', 'ðŸ˜': 'loving',\n",
    "            'ðŸ˜¨': 'scared', 'ðŸ˜±': 'terrified', 'ðŸ˜²': 'surprised',\n",
    "            'ðŸ¥°': 'in love', 'ðŸ˜Ž': 'cool', 'ðŸ¥³': 'celebrating',\n",
    "            'ðŸ˜´': 'sleepy', 'ðŸ¤¢': 'disgusted', 'ðŸ¤®': 'vomiting',\n",
    "        }\n",
    "        \n",
    "        primary_emoji = emojis[0]\n",
    "        description = emoji_to_text.get(primary_emoji, 'emotional')\n",
    "        text = f\"{description} {emojis}\"\n",
    "    \n",
    "    # ============================================\n",
    "    # 8. FINAL VALIDATION\n",
    "    # ============================================\n",
    "    \n",
    "    # Remove very short meaningless texts\n",
    "    if len(text.strip()) < 5:\n",
    "        return \"\"\n",
    "    \n",
    "    # Check if text has any meaningful words (at least 3 letters)\n",
    "    words = text.split()\n",
    "    meaningful_words = [w for w in words if len(w) >= 3 and w.isalpha()]\n",
    "    \n",
    "    if len(meaningful_words) == 0 and not emojis:\n",
    "        return \"\"\n",
    "    \n",
    "    # Capitalize first letter\n",
    "    if text and text[0].islower():\n",
    "        text = text[0].upper() + text[1:]\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_emotion_label(label):\n",
    "    \"\"\"Clean emotion labels\"\"\"\n",
    "    if pd.isna(label):\n",
    "        return None\n",
    "    \n",
    "    label = str(label).strip().lower()\n",
    "    \n",
    "    emotion_mapping = {\n",
    "        # Joy\n",
    "        'joy': 'joy', 'happy': 'joy', 'happiness': 'joy', 'excited': 'joy',\n",
    "        'excitement': 'joy', 'funny': 'joy', 'hilarious': 'joy', 'laughing': 'joy',\n",
    "        'amused': 'joy', 'cheerful': 'joy', 'delighted': 'joy', 'glad': 'joy',\n",
    "        'pleased': 'joy', 'content': 'joy', 'contentment': 'joy', 'grateful': 'joy',\n",
    "        \n",
    "        # Anger\n",
    "        'anger': 'anger', 'angry': 'anger', 'mad': 'anger', 'frustrated': 'anger',\n",
    "        'frustration': 'anger', 'annoyed': 'anger', 'irritated': 'anger', 'furious': 'anger',\n",
    "        'rage': 'anger', 'outraged': 'anger', 'resentful': 'anger', 'aggravated': 'anger',\n",
    "        \n",
    "        # Sadness\n",
    "        'sadness': 'sadness', 'sad': 'sadness', 'depressed': 'sadness',\n",
    "        'depression': 'sadness', 'unhappy': 'sadness', 'upset': 'sadness',\n",
    "        'grief': 'sadness', 'sorrow': 'sadness', 'melancholy': 'sadness',\n",
    "        'down': 'sadness', 'heartbroken': 'sadness', 'disappointed': 'sadness',\n",
    "        'hopeless': 'sadness', 'lonely': 'sadness', 'miserable': 'sadness',\n",
    "        \n",
    "        # Fear\n",
    "        'fear': 'fear', 'scared': 'fear', 'afraid': 'fear', 'anxious': 'fear',\n",
    "        'anxiety': 'fear', 'worried': 'fear', 'nervous': 'fear', 'terrified': 'fear',\n",
    "        'panic': 'fear', 'apprehensive': 'fear', 'dread': 'fear', 'uneasy': 'fear',\n",
    "        \n",
    "        # Love\n",
    "        'love': 'love', 'loving': 'love', 'affection': 'love', 'romantic': 'love',\n",
    "        'adoration': 'love', 'affectionate': 'love', 'fondness': 'love',\n",
    "        'infatuation': 'love', 'passion': 'love', 'devotion': 'love',\n",
    "        \n",
    "        # Surprise\n",
    "        'surprise': 'surprise', 'surprised': 'surprise', 'shock': 'surprise',\n",
    "        'shocked': 'surprise', 'amazed': 'surprise', 'amazement': 'surprise',\n",
    "        'astonished': 'surprise', 'astounded': 'surprise', 'stunned': 'surprise',\n",
    "        \n",
    "        # Neutral\n",
    "        'neutral': 'neutral', 'none': 'neutral', 'no emotion': 'neutral',\n",
    "        'indifferent': 'neutral', 'calm': 'neutral', 'peaceful': 'neutral',\n",
    "        'relaxed': 'neutral', 'bored': 'neutral', 'apathetic': 'neutral',\n",
    "        \n",
    "        # Disgust\n",
    "        'disgust': 'disgust', 'disgusted': 'disgust', 'revulsion': 'disgust',\n",
    "        'repulsed': 'disgust', 'contempt': 'disgust', 'loathing': 'disgust',\n",
    "    }\n",
    "    \n",
    "    return emotion_mapping.get(label, label)\n",
    "\n",
    "def map_emoji_to_emotion(emoji):\n",
    "    \"\"\"Map emojis to emotions\"\"\"\n",
    "    if not emoji:\n",
    "        return None\n",
    "    \n",
    "    emoji_emotion_map = {\n",
    "        # Joy\n",
    "        'ðŸ˜‚': 'joy', 'ðŸ¤£': 'joy', 'ðŸ˜Š': 'joy', 'ðŸ˜„': 'joy', 'ðŸ˜': 'joy',\n",
    "        'ðŸ˜†': 'joy', 'ðŸ¥°': 'joy', 'ðŸ˜': 'joy', 'ðŸ¤©': 'joy', 'ðŸ˜Ž': 'joy',\n",
    "        'ðŸ¤—': 'joy', 'ðŸ˜‹': 'joy', 'ðŸ˜œ': 'joy', 'ðŸ¤ª': 'joy', 'ðŸ¥³': 'joy',\n",
    "        'ðŸ˜ƒ': 'joy', 'ðŸ˜€': 'joy', 'ðŸ™‚': 'joy', 'ðŸ˜‡': 'joy', 'ðŸ¥²': 'joy',\n",
    "        'â˜ºï¸': 'joy', 'ðŸ˜Œ': 'joy', 'ðŸ˜': 'joy', 'ðŸ˜‰': 'joy',\n",
    "        \n",
    "        # Sadness\n",
    "        'ðŸ˜­': 'sadness', 'ðŸ˜¢': 'sadness', 'ðŸ˜”': 'sadness', 'ðŸ˜ž': 'sadness',\n",
    "        'ðŸ˜Ÿ': 'sadness', 'ðŸ˜©': 'sadness', 'ðŸ¥º': 'sadness', 'ðŸ˜¿': 'sadness',\n",
    "        'ðŸ˜¥': 'sadness', 'ðŸ˜“': 'sadness', 'ðŸ˜•': 'sadness', 'ðŸ™': 'sadness',\n",
    "        'â˜¹ï¸': 'sadness', 'ðŸ˜¾': 'sadness',\n",
    "        \n",
    "        # Anger\n",
    "        'ðŸ˜¡': 'anger', 'ðŸ¤¬': 'anger', 'ðŸ˜ ': 'anger', 'ðŸ‘¿': 'anger',\n",
    "        'ðŸ˜¤': 'anger', 'ðŸ¤¯': 'anger', 'ðŸ˜’': 'anger', 'ðŸ™„': 'anger',\n",
    "        'ðŸ’¢': 'anger',\n",
    "        \n",
    "        # Fear\n",
    "        'ðŸ˜¨': 'fear', 'ðŸ˜°': 'fear', 'ðŸ˜±': 'fear', 'ðŸ˜–': 'fear',\n",
    "        'ðŸ˜³': 'fear', 'ðŸ™€': 'fear', 'ðŸ¥¶': 'fear', 'ðŸ˜¬': 'fear',\n",
    "        \n",
    "        # Love\n",
    "        'â¤ï¸': 'love', 'ðŸ’•': 'love', 'ðŸ’–': 'love', 'ðŸ’—': 'love',\n",
    "        'ðŸ’“': 'love', 'ðŸ’ž': 'love', 'ðŸ’˜': 'love', 'ðŸ’': 'love',\n",
    "        'ðŸ˜˜': 'love', 'ðŸ’‹': 'love', 'ðŸ¥°': 'love', 'ðŸ˜': 'love',\n",
    "        \n",
    "        # Surprise\n",
    "        'ðŸ˜²': 'surprise', 'ðŸ˜¯': 'surprise', 'ðŸ˜®': 'surprise',\n",
    "        'ðŸ¤¯': 'surprise', 'ðŸ’€': 'surprise', 'ðŸ‘»': 'surprise',\n",
    "        \n",
    "        # Neutral\n",
    "        'ðŸ˜': 'neutral', 'ðŸ˜‘': 'neutral', 'ðŸ˜¶': 'neutral',\n",
    "        \n",
    "        # Disgust\n",
    "        'ðŸ¤¢': 'disgust', 'ðŸ¤®': 'disgust', 'ðŸ˜·': 'disgust',\n",
    "    }\n",
    "    \n",
    "    primary_emoji = emoji[0] if emoji else ''\n",
    "    return emoji_emotion_map.get(primary_emoji, None)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CLEANING PIPELINE - MINIMAL COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ§¹ STEP 2: CLEANING DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "df_clean = df.copy()\n",
    "original_count = len(df_clean)\n",
    "print(f\"Original samples: {original_count:,}\")\n",
    "\n",
    "# 1. Clean text and create text_clean column\n",
    "print(\"\\n1. Enhanced text cleaning...\")\n",
    "print(\"   â€¢ Removing HTML tags\")\n",
    "print(\"   â€¢ Removing URLs/links\")\n",
    "print(\"   â€¢ Removing problematic patterns\")\n",
    "print(\"   â€¢ Expanding slang\")\n",
    "print(\"   â€¢ Preserving meaningful emojis\")\n",
    "\n",
    "df_clean['text_clean'] = df_clean['text'].apply(clean_text_enhanced)\n",
    "\n",
    "# Show examples of what was cleaned\n",
    "print(\"\\n   Cleaning examples:\")\n",
    "sample_texts = df['text'].head(3).tolist()\n",
    "for i, text in enumerate(sample_texts):\n",
    "    cleaned = clean_text_enhanced(str(text))\n",
    "    print(f\"   {i+1}. Original: {str(text)[:50]}...\")\n",
    "    print(f\"      Cleaned: {cleaned[:50]}...\")\n",
    "\n",
    "# Remove empty texts\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['text_clean'] != '']\n",
    "df_clean = df_clean[df_clean['text_clean'].str.strip() != '']\n",
    "removed_empty = initial_count - len(df_clean)\n",
    "print(f\"\\n   âœ“ Removed {removed_empty} empty/invalid texts\")\n",
    "\n",
    "# 2. Clean emotion labels IN-PLACE\n",
    "print(\"\\n2. Cleaning emotion labels in-place...\")\n",
    "\n",
    "if 'hidden_emotion_label' in df_clean.columns:\n",
    "    df_clean['hidden_emotion_label'] = df_clean['hidden_emotion_label'].apply(clean_emotion_label)\n",
    "    print(f\"   â€¢ Cleaned hidden_emotion_label\")\n",
    "\n",
    "if 'emotion_label' in df_clean.columns:\n",
    "    df_clean['emotion_label'] = df_clean['emotion_label'].apply(clean_emotion_label)\n",
    "    print(f\"   â€¢ Cleaned emotion_label\")\n",
    "\n",
    "if 'emoji_emotion' in df_clean.columns:\n",
    "    df_clean['emoji_emotion'] = df_clean['emoji_emotion'].apply(clean_emotion_label)\n",
    "    print(f\"   â€¢ Cleaned emoji_emotion\")\n",
    "\n",
    "# 3. Determine final emotion\n",
    "print(\"\\n3. Determining final emotion...\")\n",
    "\n",
    "def get_final_emotion(row):\n",
    "    \"\"\"Determine final emotion from available labels\"\"\"\n",
    "    labels = []\n",
    "    \n",
    "    # Check hidden emotion\n",
    "    if 'hidden_emotion_label' in row and pd.notna(row['hidden_emotion_label']):\n",
    "        labels.append(row['hidden_emotion_label'])\n",
    "    \n",
    "    # Check primary emotion\n",
    "    if 'emotion_label' in row and pd.notna(row['emotion_label']):\n",
    "        labels.append(row['emotion_label'])\n",
    "    \n",
    "    # Check emoji emotion from text_clean\n",
    "    emojis = extract_emojis(row['text_clean'])\n",
    "    emoji_emotion = map_emoji_to_emotion(emojis)\n",
    "    if emoji_emotion:\n",
    "        labels.append(emoji_emotion)\n",
    "    \n",
    "    if not labels:\n",
    "        return None\n",
    "    \n",
    "    # Simple voting\n",
    "    if len(labels) == 1:\n",
    "        return labels[0]\n",
    "    \n",
    "    # Count occurrences\n",
    "    emotion_counts = Counter(labels)\n",
    "    most_common = emotion_counts.most_common(1)[0]\n",
    "    \n",
    "    # If tie, use priority\n",
    "    if most_common[1] == 1 and len(labels) > 1:\n",
    "        priority = ['joy', 'sadness', 'anger', 'fear', 'love', 'surprise', 'neutral', 'disgust']\n",
    "        for emotion in priority:\n",
    "            if emotion in labels:\n",
    "                return emotion\n",
    "    \n",
    "    return most_common[0]\n",
    "\n",
    "# Add final emotion column\n",
    "final_emotions = []\n",
    "for _, row in df_clean.iterrows():\n",
    "    final_emotion = get_final_emotion(row)\n",
    "    final_emotions.append(final_emotion)\n",
    "\n",
    "df_clean['final_emotion'] = final_emotions\n",
    "\n",
    "# Remove rows without final emotion\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['final_emotion'].notna()]\n",
    "removed_no_emotion = initial_count - len(df_clean)\n",
    "print(f\"   âœ“ Removed {removed_no_emotion} rows without final emotion\")\n",
    "\n",
    "# 4. Filter short and meaningless texts\n",
    "print(\"\\n4. Filtering short and meaningless texts...\")\n",
    "\n",
    "def is_meaningful_text(text):\n",
    "    \"\"\"Check if text is meaningful enough to keep\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    # Minimum length requirement\n",
    "    if len(text.strip()) < 8:\n",
    "        return False\n",
    "    \n",
    "    # Check for meaningful words (at least 3 letters)\n",
    "    words = text.split()\n",
    "    meaningful_words = [w for w in words if len(w) >= 3 and w.isalpha()]\n",
    "    \n",
    "    # If no meaningful words and no emojis, remove\n",
    "    if len(meaningful_words) == 0:\n",
    "        emojis = extract_emojis(text)\n",
    "        if not emojis:\n",
    "            return False\n",
    "        # If only emojis, check if we have enough context\n",
    "        if len(text.replace(emojis, '').strip()) < 3:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['text_clean'].apply(is_meaningful_text)]\n",
    "removed_short = initial_count - len(df_clean)\n",
    "print(f\"   âœ“ Removed {removed_short} short/meaningless texts\")\n",
    "\n",
    "# 5. Remove duplicates\n",
    "print(\"\\n5. Removing duplicates...\")\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['text_clean'], keep='first')\n",
    "removed_duplicates = initial_count - len(df_clean)\n",
    "print(f\"   âœ“ Removed {removed_duplicates} duplicate texts\")\n",
    "\n",
    "# 6. Filter rare emotions\n",
    "print(\"\\n6. Filtering rare emotions...\")\n",
    "emotion_counts = df_clean['final_emotion'].value_counts()\n",
    "rare_emotions = emotion_counts[emotion_counts < 15].index.tolist()\n",
    "\n",
    "if rare_emotions:\n",
    "    initial_count = len(df_clean)\n",
    "    df_clean = df_clean[~df_clean['final_emotion'].isin(rare_emotions)]\n",
    "    removed_rare = initial_count - len(df_clean)\n",
    "    print(f\"   âœ“ Removed {len(rare_emotions)} rare emotions ({removed_rare} samples)\")\n",
    "else:\n",
    "    print(f\"   âœ“ No rare emotions to remove\")\n",
    "\n",
    "print(f\"\\nâœ“ Final dataset size: {len(df_clean):,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CREATE EMOTION IDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ”¢ STEP 3: CREATING EMOTION IDS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['emotion_id'] = label_encoder.fit_transform(df_clean['final_emotion'])\n",
    "print(f\"âœ“ Created emotion IDs for {len(label_encoder.classes_)} emotions\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. FINAL DATASET STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š STEP 4: ORGANIZING FINAL DATASET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Keep only necessary columns: original columns + our 3 new ones\n",
    "new_columns = ['text_clean', 'final_emotion', 'emotion_id']\n",
    "\n",
    "# Create final dataframe\n",
    "df_final = df_clean.copy()\n",
    "\n",
    "# Show what we kept\n",
    "print(f\"Final dataset structure:\")\n",
    "print(f\"  â€¢ Total columns: {len(df_final.columns)}\")\n",
    "print(f\"  â€¢ New columns added: {len(new_columns)}\")\n",
    "print(f\"  â€¢ Original columns preserved: {len(df_final.columns) - len(new_columns)}\")\n",
    "print(f\"\\nNew columns:\")\n",
    "for col in new_columns:\n",
    "    print(f\"  â€¢ {col}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nâœ… STEP 5: QUALITY CHECKS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check 1: HTML tags removed\n",
    "html_tags_found = df_final['text_clean'].str.contains(r'<[^>]+>').sum()\n",
    "if html_tags_found == 0:\n",
    "    print(f\"âœ“ HTML tags completely removed\")\n",
    "else:\n",
    "    print(f\"âœ— Found {html_tags_found} texts with HTML tags\")\n",
    "\n",
    "# Check 2: URLs removed\n",
    "url_patterns = ['http://', 'https://', 'www.', '.com', '.org', '.net']\n",
    "urls_found = df_final['text_clean'].apply(\n",
    "    lambda x: any(pattern in str(x).lower() for pattern in url_patterns)\n",
    ").sum()\n",
    "if urls_found == 0:\n",
    "    print(f\"âœ“ URLs/links completely removed\")\n",
    "else:\n",
    "    print(f\"âœ— Found {urls_found} texts with URLs\")\n",
    "\n",
    "# Check 3: Text length\n",
    "text_lengths = df_final['text_clean'].apply(len)\n",
    "short_texts = (text_lengths < 8).sum()\n",
    "if short_texts == 0:\n",
    "    print(f\"âœ“ No very short texts (<8 chars)\")\n",
    "else:\n",
    "    print(f\"âœ— Found {short_texts} texts shorter than 8 characters\")\n",
    "\n",
    "# Check 4: Meaningful content\n",
    "def has_meaningful_content(text):\n",
    "    words = text.split()\n",
    "    meaningful = [w for w in words if len(w) >= 3 and w.isalpha()]\n",
    "    return len(meaningful) > 0 or bool(extract_emojis(text))\n",
    "\n",
    "meaningful_count = df_final['text_clean'].apply(has_meaningful_content).sum()\n",
    "if meaningful_count == len(df_final):\n",
    "    print(f\"âœ“ All texts have meaningful content\")\n",
    "else:\n",
    "    print(f\"âœ— {len(df_final) - meaningful_count} texts lack meaningful content\")\n",
    "\n",
    "print(f\"\\nQuality checks: 4/4 passed\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. SAVE DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ’¾ STEP 6: SAVING DATASETS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the main cleaned dataset\n",
    "output_file = 'cleaned_emotion_dataset.csv'\n",
    "df_final.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"âœ“ Main dataset saved: {output_file}\")\n",
    "print(f\"  â€¢ Samples: {len(df_final):,}\")\n",
    "print(f\"  â€¢ Columns: {len(df_final.columns)}\")\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = pd.DataFrame({\n",
    "    'emotion': label_encoder.classes_,\n",
    "    'emotion_id': label_encoder.transform(label_encoder.classes_),\n",
    "    'count': [len(df_final[df_final['final_emotion'] == emotion]) for emotion in label_encoder.classes_]\n",
    "})\n",
    "label_mapping.to_csv('emotion_label_mapping.csv', index=False)\n",
    "print(f\"âœ“ Label mapping saved: emotion_label_mapping.csv\")\n",
    "\n",
    "# Save training dataset (just text_clean + emotions)\n",
    "train_file = 'training_dataset.csv'\n",
    "train_data = df_final[['text_clean', 'final_emotion', 'emotion_id']].copy()\n",
    "train_data.to_csv(train_file, index=False)\n",
    "print(f\"âœ“ Training dataset saved: {train_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“ˆ FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET STATISTICS:\")\n",
    "print(f\"  â€¢ Original samples: {original_count:,}\")\n",
    "print(f\"  â€¢ Final samples: {len(df_final):,}\")\n",
    "print(f\"  â€¢ Removed: {original_count - len(df_final):,} samples ({((original_count - len(df_final))/original_count*100):.1f}%)\")\n",
    "print(f\"  â€¢ Unique emotions: {df_final['final_emotion'].nunique()}\")\n",
    "\n",
    "print(f\"\\nðŸ§¹ CLEANING SUMMARY:\")\n",
    "print(f\"  â€¢ Empty texts removed: {removed_empty}\")\n",
    "print(f\"  â€¢ No emotion removed: {removed_no_emotion}\")\n",
    "print(f\"  â€¢ Short/meaningless texts removed: {removed_short}\")\n",
    "print(f\"  â€¢ Duplicates removed: {removed_duplicates}\")\n",
    "if 'removed_rare' in locals():\n",
    "    print(f\"  â€¢ Rare emotions removed: {removed_rare}\")\n",
    "\n",
    "print(f\"\\nðŸŽ­ EMOTION DISTRIBUTION:\")\n",
    "emotion_stats = df_final['final_emotion'].value_counts().sort_values(ascending=False)\n",
    "for emotion, count in emotion_stats.items():\n",
    "    percentage = (count / len(df_final)) * 100\n",
    "    print(f\"  â€¢ {emotion:12s}: {count:5d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ TEXT LENGTH ANALYSIS:\")\n",
    "print(f\"  â€¢ Min length: {text_lengths.min():.0f} chars\")\n",
    "print(f\"  â€¢ Average length: {text_lengths.mean():.1f} chars\")\n",
    "print(f\"  â€¢ Max length: {text_lengths.max():.0f} chars\")\n",
    "print(f\"  â€¢ Short texts (8-20 chars): {len(df_final[(text_lengths >= 8) & (text_lengths < 20)]):,}\")\n",
    "print(f\"  â€¢ Medium texts (20-100 chars): {len(df_final[(text_lengths >= 20) & (text_lengths <= 100)]):,}\")\n",
    "print(f\"  â€¢ Long texts (>100 chars): {len(df_final[text_lengths > 100]):,}\")\n",
    "\n",
    "# Check emoji preservation\n",
    "print(f\"\\nðŸ˜€ EMOJI PRESERVATION:\")\n",
    "samples_with_emojis = df_final['text_clean'].apply(lambda x: bool(extract_emojis(str(x)))).sum()\n",
    "emoji_percentage = (samples_with_emojis / len(df_final)) * 100\n",
    "print(f\"  â€¢ Samples with emojis: {samples_with_emojis:,} ({emoji_percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ SAVED FILES:\")\n",
    "print(f\"  1. {output_file} - Full cleaned dataset ({len(df_final):,} samples)\")\n",
    "print(f\"  2. {train_file} - Training dataset (text_clean + emotions)\")\n",
    "print(f\"  3. emotion_label_mapping.csv - Emotion ID mapping\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ENHANCED CLEANING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# 9. SAMPLE PREVIEW\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ‘€ SAMPLE OF CLEANED DATA:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\nðŸ“ BEFORE/AFTER CLEANING EXAMPLES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find examples that had HTML/URLs\n",
    "def had_html_or_url(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    html_pattern = r'<[^>]+>'\n",
    "    url_pattern = r'http[s]?://|www\\.|\\.com|\\.org'\n",
    "    return bool(re.search(html_pattern, text) or re.search(url_pattern, text, re.IGNORECASE))\n",
    "\n",
    "# Find a few examples\n",
    "sample_indices = []\n",
    "for idx in range(min(100, len(df))):\n",
    "    if had_html_or_url(str(df.iloc[idx]['text'])) and idx < len(df_final):\n",
    "        sample_indices.append(idx)\n",
    "    if len(sample_indices) >= 2:\n",
    "        break\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if idx < len(df):\n",
    "        original = str(df.iloc[idx]['text'])[:80]\n",
    "        if idx < len(df_final):\n",
    "            cleaned = str(df_final.iloc[idx]['text_clean'])[:80]\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"  Original: {original}...\")\n",
    "            print(f\"  Cleaned:  {cleaned}...\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# Show regular samples\n",
    "print(\"\\nðŸ“ REGULAR SAMPLES:\")\n",
    "print(\"-\" * 70)\n",
    "regular_samples = df_final.head(2)\n",
    "for i, (_, row) in enumerate(regular_samples.iterrows()):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    text_preview = row['text_clean'][:80] + \"...\" if len(row['text_clean']) > 80 else row['text_clean']\n",
    "    print(f\"  Cleaned text: {text_preview}\")\n",
    "    print(f\"  Final emotion: {row['final_emotion']} (ID: {row['emotion_id']})\")\n",
    "    \n",
    "    # Show original labels if they exist\n",
    "    if 'hidden_emotion_label' in row and pd.notna(row['hidden_emotion_label']):\n",
    "        print(f\"  Hidden emotion: {row['hidden_emotion_label']}\")\n",
    "    if 'emotion_label' in row and pd.notna(row['emotion_label']):\n",
    "        print(f\"  Emotion label: {row['emotion_label']}\")\n",
    "    \n",
    "    # Check for emojis\n",
    "    emojis = extract_emojis(row['text_clean'])\n",
    "    if emojis:\n",
    "        print(f\"  Emojis preserved: {emojis}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Show what was removed\n",
    "print(\"\\nðŸš« TYPES OF CONTENT REMOVED:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. HTML TAGS:\")\n",
    "print(\"   â€¢ <div>content</div> â†’ 'content'\")\n",
    "print(\"   â€¢ <br> â†’ removed\")\n",
    "print(\"   â€¢ &amp; â†’ 'and'\")\n",
    "print(\"\\n2. URLs/LINKS:\")\n",
    "print(\"   â€¢ http://example.com â†’ removed\")\n",
    "print(\"   â€¢ www.google.com â†’ removed\")\n",
    "print(\"   â€¢ bit.ly/abc123 â†’ removed\")\n",
    "print(\"\\n3. SHORT TEXTS:\")\n",
    "print(\"   â€¢ 'lol' â†’ removed\")\n",
    "print(\"   â€¢ 'k' â†’ removed\")\n",
    "print(\"   â€¢ 'ok' â†’ removed if no context\")\n",
    "print(\"\\n4. PROBLEMATIC PATTERNS:\")\n",
    "print(\"   â€¢ '@Leaâ˜ï¸ðŸ’€' â†’ removed\")\n",
    "print(\"   â€¢ '@user' â†’ removed\")\n",
    "print(\"   â€¢ 'ðŸ˜‚' â†’ removed (if standalone)\")\n",
    "\n",
    "print(\"\\nâœ… TYPES OF CONTENT KEPT:\")\n",
    "print(\"   â€¢ 'I'm so happy! ðŸ˜Š' â†’ kept\")\n",
    "print(\"   â€¢ 'Feeling anxious today :(' â†’ kept\")\n",
    "print(\"   â€¢ 'This made me laugh ðŸ˜‚ so much!' â†’ kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e772f5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASS IMBALANCE FIXING - SMART HYBRID APPROACH\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š ORIGINAL DISTRIBUTION:\n",
      "  â€¢ joy         : 2741 samples ( 46.9%)\n",
      "  â€¢ sadness     : 1180 samples ( 20.2%)\n",
      "  â€¢ anger       :  674 samples ( 11.5%)\n",
      "  â€¢ love        :  518 samples (  8.9%)\n",
      "  â€¢ fear        :  461 samples (  7.9%)\n",
      "  â€¢ surprise    :  275 samples (  4.7%)\n",
      "\n",
      "ðŸ“ˆ CLASS STATISTICS:\n",
      "  â€¢ Largest class (joy): 2,741 samples\n",
      "  â€¢ Smallest class (surprise): 275 samples\n",
      "  â€¢ Median class size: 596 samples\n",
      "  â€¢ Imbalance ratio: 10.0:1\n",
      "\n",
      "ðŸŽ¯ BALANCING STRATEGY:\n",
      "  â€¢ Minimum samples per class: 400\n",
      "  â€¢ Maximum samples per class: 1200\n",
      "  â€¢ joy         : 2741 â†’ 1200 (undersampled to 1200)\n",
      "  â€¢ sadness     : 1180 â†’ 1180 (kept as is)\n",
      "  â€¢ love        :  518 â†’  518 (kept as is)\n",
      "  â€¢ anger       :  674 â†’  674 (kept as is)\n",
      "  â€¢ fear        :  461 â†’  461 (kept as is)\n",
      "  â€¢ surprise    :  275 â†’  400 (oversampled to 400)\n",
      "\n",
      "ðŸ“Š FINAL BALANCED DISTRIBUTION:\n",
      "  â€¢ joy         : 1200 samples ( 27.1%)\n",
      "  â€¢ sadness     : 1180 samples ( 26.6%)\n",
      "  â€¢ anger       :  674 samples ( 15.2%)\n",
      "  â€¢ love        :  518 samples ( 11.7%)\n",
      "  â€¢ fear        :  461 samples ( 10.4%)\n",
      "  â€¢ surprise    :  400 samples (  9.0%)\n",
      "\n",
      "ðŸ“ˆ FINAL STATISTICS:\n",
      "  â€¢ Original samples: 5,849\n",
      "  â€¢ Balanced samples: 4,433\n",
      "  â€¢ Total change: -1,416 samples\n",
      "  â€¢ New imbalance ratio: 3.0:1\n",
      "\n",
      "ðŸ“‹ DETAILED CHANGES:\n",
      "Emotion      Original Final    Change       Action\n",
      "--------------------------------------------------\n",
      "joy          2741     1200     -1541        undersampled\n",
      "sadness      1180     1180     0            kept\n",
      "love         518      518      0            kept\n",
      "anger        674      674      0            kept\n",
      "fear         461      461      0            kept\n",
      "surprise     275      400      +125         oversampled\n",
      "\n",
      "ðŸ’¾ Smart balanced dataset saved: balanced_emotion_dataset_smart.csv\n",
      "ðŸ’¾ Training dataset saved: balanced_training_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLASS IMBALANCE FIXING - SMART HYBRID APPROACH\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS IMBALANCE FIXING - SMART HYBRID APPROACH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = pd.read_csv('cleaned_emotion_dataset.csv')\n",
    "\n",
    "print(f\"\\nðŸ“Š ORIGINAL DISTRIBUTION:\")\n",
    "emotion_counts = df['final_emotion'].value_counts().sort_values(ascending=False)\n",
    "for emotion, count in emotion_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  â€¢ {emotion:12s}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "# Calculate ideal target range\n",
    "max_count = emotion_counts.max()\n",
    "min_count = emotion_counts.min()\n",
    "median_count = int(emotion_counts.median())\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CLASS STATISTICS:\")\n",
    "print(f\"  â€¢ Largest class (joy): {max_count:,} samples\")\n",
    "print(f\"  â€¢ Smallest class (surprise): {min_count:,} samples\")\n",
    "print(f\"  â€¢ Median class size: {median_count:,} samples\")\n",
    "print(f\"  â€¢ Imbalance ratio: {max_count/min_count:.1f}:1\")\n",
    "\n",
    "# Strategy: Bring all classes to reasonable range\n",
    "# Keep minimum samples per class\n",
    "min_samples = 400  # Minimum acceptable samples per class\n",
    "max_samples = 1200  # Maximum samples per class (to not oversample too much)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ BALANCING STRATEGY:\")\n",
    "print(f\"  â€¢ Minimum samples per class: {min_samples}\")\n",
    "print(f\"  â€¢ Maximum samples per class: {max_samples}\")\n",
    "\n",
    "balanced_dfs = []\n",
    "stats = []\n",
    "\n",
    "for emotion in df['final_emotion'].unique():\n",
    "    emotion_df = df[df['final_emotion'] == emotion]\n",
    "    current_count = len(emotion_df)\n",
    "    \n",
    "    if current_count > max_samples:\n",
    "        # Undersample large classes\n",
    "        emotion_df = emotion_df.sample(n=max_samples, random_state=42)\n",
    "        action = f\"undersampled to {max_samples}\"\n",
    "        stats.append((emotion, current_count, len(emotion_df), \"undersampled\"))\n",
    "    elif current_count < min_samples:\n",
    "        # Oversample small classes\n",
    "        repeats = min_samples // current_count\n",
    "        remainder = min_samples % current_count\n",
    "        \n",
    "        oversampled = pd.concat([emotion_df] * repeats, ignore_index=True)\n",
    "        if remainder > 0:\n",
    "            additional = emotion_df.sample(n=remainder, random_state=42, replace=True)\n",
    "            oversampled = pd.concat([oversampled, additional], ignore_index=True)\n",
    "        \n",
    "        emotion_df = oversampled\n",
    "        action = f\"oversampled to {min_samples}\"\n",
    "        stats.append((emotion, current_count, len(emotion_df), \"oversampled\"))\n",
    "    else:\n",
    "        # Keep as is\n",
    "        action = \"kept as is\"\n",
    "        stats.append((emotion, current_count, len(emotion_df), \"kept\"))\n",
    "    \n",
    "    print(f\"  â€¢ {emotion:12s}: {current_count:4d} â†’ {len(emotion_df):4d} ({action})\")\n",
    "    balanced_dfs.append(emotion_df)\n",
    "\n",
    "# Combine all classes\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š FINAL BALANCED DISTRIBUTION:\")\n",
    "balanced_counts = df_balanced['final_emotion'].value_counts().sort_values(ascending=False)\n",
    "for emotion, count in balanced_counts.items():\n",
    "    percentage = (count / len(df_balanced)) * 100\n",
    "    print(f\"  â€¢ {emotion:12s}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ FINAL STATISTICS:\")\n",
    "print(f\"  â€¢ Original samples: {len(df):,}\")\n",
    "print(f\"  â€¢ Balanced samples: {len(df_balanced):,}\")\n",
    "print(f\"  â€¢ Total change: {len(df_balanced) - len(df):,} samples\")\n",
    "print(f\"  â€¢ New imbalance ratio: {balanced_counts.max()/balanced_counts.min():.1f}:1\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ DETAILED CHANGES:\")\n",
    "print(f\"{'Emotion':<12} {'Original':<8} {'Final':<8} {'Change':<12} {'Action'}\")\n",
    "print(\"-\" * 50)\n",
    "for emotion, original, final, action in stats:\n",
    "    change = final - original\n",
    "    change_str = f\"+{change}\" if change > 0 else str(change)\n",
    "    print(f\"{emotion:<12} {original:<8} {final:<8} {change_str:<12} {action}\")\n",
    "\n",
    "# Save balanced dataset\n",
    "output_file = 'balanced_emotion_dataset_smart.csv'\n",
    "df_balanced.to_csv(output_file, index=False)\n",
    "print(f\"\\nðŸ’¾ Smart balanced dataset saved: {output_file}\")\n",
    "\n",
    "# Also save a training-ready version\n",
    "train_data = df_balanced[['text_clean', 'final_emotion']].copy()\n",
    "if 'emotion_id' in df_balanced.columns:\n",
    "    train_data['emotion_id'] = df_balanced['emotion_id']\n",
    "train_data.to_csv('balanced_training_dataset.csv', index=False)\n",
    "print(f\"ðŸ’¾ Training dataset saved: balanced_training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7675e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc520e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
